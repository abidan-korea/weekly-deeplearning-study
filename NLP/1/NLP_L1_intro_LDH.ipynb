{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "british-piece",
   "metadata": {},
   "source": [
    "# 순서\n",
    "\n",
    "1.결론\n",
    "\n",
    "2.LMS 노드 내용 정리\n",
    "\n",
    "3.코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-reputation",
   "metadata": {},
   "source": [
    "## 1.결론\n",
    "\n",
    "분산표현을 구현하기 위한 방법 : Tokenizing\n",
    "\n",
    "1.Token 단위 : 1)word, 2)sub-word(BPE, WPM, SentencePiece), 3)형태소\n",
    "\n",
    "2.Tokenizing 방법 : BPE, WPM, SentencePice\n",
    "\n",
    "3.Token to Vector 방법 : word2vec(CBOW, Skip-gram), FastText, ELMo\n",
    "\n",
    "희소표현 : 1)indices, 2)one-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-granny",
   "metadata": {},
   "source": [
    "## 2.노드 내용 정리\n",
    "\n",
    "\n",
    "#### 학습 목표\n",
    "\n",
    "1.분산표현에 대한 직관적 이해를 얻는다.\n",
    "\n",
    "2.문장 데이터를 정제하는 방법을 배운다.\n",
    "\n",
    "3.토큰화의 여러 가지 기법들을 배운다.\n",
    "\n",
    "4.단어 Embedding을 구축하는 방법에 대해 가볍게 맛본다\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "자연어, 인공어, 프로그래밍 언어\n",
    "문맥자유문법(context free grammer), 문맥 의존 문법(Context-sensitive grammer)\n",
    "syntaxnet, google - https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html\n",
    "real world knowledge\n",
    "\n",
    "1-2.언어모델 - 단어가 출현하게될 확률 모델, 일정한 패턴 => 통계/딥러닝 학습 => 문법/언어관습 패턴\n",
    "\n",
    "불완전한 대화\n",
    "- 문장 길이가 너무 짧거나 긴 경우\n",
    "- 채팅 데이터간 문장 시간간격이 너무 긴 경우\n",
    "- 바람직하지 않은 문장의 사용 - 욕설, 오타\n",
    "- 노이즈1) 문장부호 - replace\n",
    "- 노이즈2) 대소문자\n",
    "- 노이즈3) 특수문자\n",
    "정규표현식(re)\n",
    "\n",
    "1-3.분산표현\n",
    "분산표현 vs 희소표현\n",
    "- 희소표현 : 단어를 고차원 벡터로 변환\n",
    "- 너무 고차원은 불가 : 두 단어의 코사인 유사도를 통한 유사도 구하기\n",
    "\n",
    "분산표현 : 임베딩 레이어를 사용해 각 단어가 몇 차원의 속성을 가질질 정의\n",
    "일일이 정의할 수 없는 추상적인 속성을 256 차원 안에 골고루 분산되게 표현\n",
    "\n",
    "단어사전 구성, 활용의 문제 : 컴퓨터 vs 사람의 인식 차이\n",
    "\n",
    "1.4.토큰화 : 그녀는? 그녀 + 는?\n",
    "문장을 어느 단위까지, 어느 레벨까지 자를 것이냐? : 토큰화의 기준\n",
    "1)공백기반 토큰화 : .split()\n",
    "2)형태소 기반 토큰화 : 1)은 문제가 많아. - konlpy, khaii, komoran, 꼬꼬마, mecab\n",
    "성능비교 : https://iostream.tistory.com/144https://iostream.tistory.com/144\n",
    "\n",
    "사전에 없는 단어의 문제\n",
    "형태소 기반 토큰화 기법 : 모두 의미를 가지는 단위로 토큰을 생성함.\n",
    "빈도수 기준 상위 N 개 단어만 사용하고, 나머지는 unk 특수토큰 치환 : 손실\n",
    ": out of vacabulary 문제 : 어떻게 하냐?\n",
    "\n",
    "\n",
    "1-5. Wordpiece Model(WPM)\n",
    "preview, predict : pre- 라는 공동된 접두어 가지고 있음\n",
    "학습 시, pre+view, pre+dict 로 학습하면 더 잘 되지 않을까?\n",
    "pre : subword 같이 '한 단어를 여러 개의 subword 의 집합으로 보는 방법이 WPM'\n",
    "\n",
    "#1)BPE(byte-pair encoding)\n",
    "\n",
    "1994년 데이터 압축을 위해 생겨남.\n",
    "데이터에서 가장 많이 등장하는 바이트쌍(byte-pair)를 새로운 단어로 치환하여 압축하는 작업을 반복\n",
    "\n",
    "2015년 위 bpe 를 토큰화에 적용하자고 제안.\n",
    "모든 단어를 문자(byte)들의 집합으로 취급\n",
    "장점1 : 접두어/접미어 의미 캐치할수 있음. \n",
    "2 : 처음 등장하는 단어는 문자/알파벳 조합으로 나타내면 oov 문제 해결\n",
    "\n",
    "\n",
    "lowest : low+est 의 결합으로 표현가능.\n",
    "큰 데이터도 원하는 크기로 사전을 정의 가능함.\n",
    "\n",
    "embedding layer 는 [단어의 개수 * 임베딩 차원수] 의 weight 를 생성함\n",
    "단어의 개수가 줄어들면, 메모리 절약 가능함.\n",
    "bpe 를 적용하면 단어들을 쪼개기는 쉬우나, 조합하는데 어려움이 있다.\n",
    "\n",
    "#2)Wordpiece Model(WPM)\n",
    "\n",
    "1)byte-pair 로 쪼개고, 시작부분에 _ 를 추가함\n",
    "2)빈도수 기반이 아닌 가능도(likelihood)를 증가시키는 방향으로 문자 쌍을 합침\n",
    "\n",
    "한국어, 일본어 등 조사/어미의 활용이 높은 모델은 WPM 이 좋은 대안\n",
    "WPM은 어떤 언어든 무관하게 적용한 language-neutral, general 한 기법\n",
    "\n",
    "확률, 가능도 : https://jjangjjong.tistory.com/41\n",
    "\n",
    "Katz's Back-off Model : LM 에서 등장하지 않은 단어에 대해 확률을 할당하는 모델\n",
    "\n",
    "google bpe github : https://github.com/google/sentencepiece\n",
    "\n",
    "sentencepiece 상당히 좋으나, '차' 라는 단어가 마시는 차인지 타고다니는 차인지 의미에 대해서는 정보가 없음\n",
    "\n",
    "1-6)토큰에게 의미를 부여하기\n",
    "- word2vec, FastText, ELMo\n",
    "\n",
    "#word2vec - https://wikidocs.net/22660\n",
    "word2vec 2가지 방식 : 1CBOW, 2,Skip-gram\n",
    "논리적으로는 CBOW 가 좋아보이나, 성능은 skip-gram 이 우세함\n",
    "\n",
    "#FastText\n",
    "word2vec 은 좋은 방법이지만, 연산의 빈부격차가 존재함.\n",
    "자주 등장하지 않는 단어는 최악의 경우 1번의 연산만으로 random 하게 초기화된 상태로 알고리즘 종료\n",
    "이를 해결하기 위해 FASTTEXT 는 BPE 와 비슷한 아이디어 적용함.\n",
    "FastText 는 한 단어를 n-gram 의 집합으로 보고, 단어를 쪼개서 n-gram 에 할당된 embedding 의 평균값을 사용함\n",
    "https://brunch.co.kr/@learning/7\n",
    "\n",
    "#ELMo\n",
    "- the 1st contextualized Word embedding\n",
    "위 word2vec, FastText 는 고질적인 문제가 있음. 고정적\n",
    "사과가 먹는사과인지, 용서를 구하는 사과인지?\n",
    "즉, 단어의 의미를 정하려면, 문장의 context 가 필요하다.\n",
    "먹음직한 사과 : 먹는 사과\n",
    "무릎꿇고 사과를 하다 : 용서를 구하는 사과\n",
    "\n",
    "이런 개념을 Contextualized Word Embedding 이라고 한다.\n",
    "링크 : https://brunch.co.kr/@learning/12\n",
    "\n",
    "\n",
    "\n",
    "1-7)마무리\n",
    "데이터에 sentencepiece 적용 후 word2vec 를 적용해 유사도 비교 해보고 싶다.\n",
    "\n",
    "오늘 배운 내용 NLP 공부하게되면 계속 나올 것이다.(BPE 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-grove",
   "metadata": {},
   "source": [
    "## sentence piece\n",
    "\n",
    "https://github.com/google/sentencepiece\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-hurricane",
   "metadata": {},
   "source": [
    "### Subword NMT(Neural Machine Translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-tennessee",
   "metadata": {},
   "source": [
    "## 3.코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-chassis",
   "metadata": {},
   "source": [
    "## 1-3 분산표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-manor",
   "metadata": {},
   "source": [
    "## 공백 기반 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complex-malta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = \\\n",
    "\"\"\"\n",
    "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
    "but my teacher had been with me several weeks before i understood that everything has a name . \n",
    "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
    "some one was drawing water and my teacher placed my hand under the spout .  \n",
    "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
    "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
    "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
    "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
    "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
    "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
    "\"\"\"\n",
    "\n",
    "tokens = corpus.split()\n",
    "\n",
    "print(\"문장이 포함하는 Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-study",
   "metadata": {},
   "source": [
    "# 형태소 기반 토큰화\n",
    "\n",
    "한국어 형태소 분석기는 대표적으로 아래 두 가지가 사용됩니다.\n",
    "\n",
    "KoNLPy: 파이썬 한국어 NLP - KoNLPy 0.4.3 documentation\n",
    "\n",
    "kakao/khaiii\n",
    "\n",
    "▲ Khaiii는 Windows를 지원하지 않습니다!\n",
    "\n",
    "KoNLPy는 내부적으로 5가지의 형태소 분석 Class를 포함하고 있습니다. Khaiii까지 총 6개나 되는 형태소 분석기들은 특수한 문장(띄어쓰기 X / 오탈자) 처리 성능, 속도 측면에서 차이를 보입니다. 천하무적인 것은 (아직은) 없으니, 각 분석기를 직접 테스트해보고 적합한 것을 선택해 사용하면 됩니다.\n",
    "\n",
    "아래 웹페이지에서 그 실험을 대신해주었네요. 정말 좋은 참고 자료입니다!\n",
    "\n",
    "한국어 형태소 분석기 성능 비교\n",
    "\n",
    "분석기\t로딩 시간 (초)\n",
    "khaiii\t0.0016\n",
    "한나눔\t0.0001\n",
    "꼬꼬마\t0.0002\n",
    "KOMORAN\t0.9542\n",
    "Open Korean Text (이하 OKT)\t0.0001\n",
    "mecab\t0.0004\n",
    "\n",
    "mecab : 속도 최우선, 품질 상위권\n",
    "KOMORAN : 자소 분리 / 오탈자 품질 보장\n",
    "한나눔, khaiii : 분석 품질 별로\n",
    "꼬꼬마 : 분석 시간 별로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sexual-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt\n",
    "\n",
    "\"\"\"\n",
    "$ pip install konlpy\n",
    "$ cd ~/aiffel/text_preprocess\n",
    "$ git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "$ cd Mecab-ko-for-Google-Colab\n",
    "$ bash install_mecab-ko_on_colab190912.sh\n",
    "\n",
    "$ sudo apt update\n",
    "$ sudo apt install default-jre\n",
    "\n",
    "khaiii 설치!\n",
    "위와 같은 초기화 코드가 실행 시 오류가 발생한다면 다음과 같은 설치 과정이 필요합니다. 원활한 설치를 위해 Khaiii 설치 과정을 다음과 같은 스크립트로 제공해 드립니다.\n",
    "\n",
    "$ sudo apt install cmake   # Khaiii의 빌드 과정을 위해 cmake를 필요로 합니다.\n",
    "$ pip install torch     # Khaiii는 구동을 위해 파이토치를 필요로 합니다. \n",
    "$ cd ~/aiffel/text_preprocess\n",
    "$ git clone https://github.com/modulabs/khaiii_wrapper.git\n",
    "$ ./install_khaiii.sh\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "passive-tuition",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'khaiii'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-65c8689d2132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkhaiii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkhaiii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKhaiiiApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'khaiii'"
     ]
    }
   ],
   "source": [
    "import khaiii\n",
    "\n",
    "api = khaiii.KhaiiiApi()\n",
    "api.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acting-mouth",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'java.lang.String' object has no attribute 'splitlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d526810d53eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[{}] \\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkor_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/konlpy/tag/_hannanum.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, ntags, flatten, join)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ntags in [9, 22]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/konlpy/tag/_hannanum.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(result, flatten, join)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0melems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'java.lang.String' object has no attribute 'splitlines'"
     ]
    }
   ],
   "source": [
    "tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt()] #,Khaiii()]\n",
    "\n",
    "kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n",
    "\n",
    "for tokenizer in tokenizer_list:\n",
    "    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-broad",
   "metadata": {},
   "source": [
    "## 1-5 WPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-glossary",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sitting-venue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Step 1\n",
      "best :  ('e', 's')\n",
      "다음 문자 쌍을 치환: es\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w es t ': 6, 'w i d es t ': 3} \n",
      "\n",
      ">> Step 2\n",
      "best :  ('es', 't')\n",
      "다음 문자 쌍을 치환: est\n",
      "변환된 Vocab:\n",
      " {'l o w ': 5, 'l o w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 3\n",
      "best :  ('l', 'o')\n",
      "다음 문자 쌍을 치환: lo\n",
      "변환된 Vocab:\n",
      " {'lo w ': 5, 'lo w e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 4\n",
      "best :  ('lo', 'w')\n",
      "다음 문자 쌍을 치환: low\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'n e w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 5\n",
      "best :  ('n', 'e')\n",
      "다음 문자 쌍을 치환: ne\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'ne w est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 6\n",
      "best :  ('ne', 'w')\n",
      "다음 문자 쌍을 치환: new\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'new est ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 7\n",
      "best :  ('new', 'est')\n",
      "다음 문자 쌍을 치환: newest\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'newest ': 6, 'w i d est ': 3} \n",
      "\n",
      ">> Step 8\n",
      "best :  ('w', 'i')\n",
      "다음 문자 쌍을 치환: wi\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'newest ': 6, 'wi d est ': 3} \n",
      "\n",
      ">> Step 9\n",
      "best :  ('wi', 'd')\n",
      "다음 문자 쌍을 치환: wid\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'newest ': 6, 'wid est ': 3} \n",
      "\n",
      ">> Step 10\n",
      "best :  ('wid', 'est')\n",
      "다음 문자 쌍을 치환: widest\n",
      "변환된 Vocab:\n",
      " {'low ': 5, 'low e r ': 2, 'newest ': 6, 'widest ': 3} \n",
      "\n",
      "Merge Vocab: ['es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid', 'widest']\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "# 임의의 데이터에 포함된 단어들입니다.\n",
    "# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n",
    "vocab = {\n",
    "    'l o w '      : 5,\n",
    "    'l o w e r '  : 2,\n",
    "    'n e w e s t ': 6,\n",
    "    'w i d e s t ': 3\n",
    "}\n",
    "\n",
    "num_merges = 10\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    단어 사전을 불러와\n",
    "    단어는 공백 단위로 쪼개어 문자 list를 만들고\n",
    "    빈도수와 쌍을 이루게 합니다. (symbols)\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    \n",
    "    for word, freq in vocab.items():   # word : l o w, # freq : 5\n",
    "        \n",
    "        symbols = word.split() # symbols : l, o, w\n",
    "        \n",
    "        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여 \n",
    "            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. \n",
    "            #print(\"get_stats, pairs : \", pairs)\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    return v_out, pair[0] + pair[1]\n",
    "\n",
    "token_vocab = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(\">> Step {0}\".format(i + 1))\n",
    "    \n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n",
    "    print(\"best : \", best)\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    print(\"다음 문자 쌍을 치환:\", merge_tok)\n",
    "    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n",
    "    \n",
    "    token_vocab.append(merge_tok)\n",
    "    \n",
    "print(\"Merge Vocab:\", token_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-rotation",
   "metadata": {},
   "source": [
    "# LMS 종료"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
